+++
title = "Back to the basic statistics: On the interaction term of logistic regression"
subtitle = ""

date = 2019-03-16T00:00:00
draft = true

authors = ["Chi Zhang"]

summary = ""



+++



As a PhD student, doing statistical advising to clinicians at our university hospital is sometimes part of the job, I also enjoy doing it (when I have time) because it allows me to see real problems and real data. It is also a very good way to learn how to interpret the model results. I intend to blog about the little pieces of basic statistics that I encounter, just to understand the modern world and its complexity a bit better. 



### A very simple logistic regression example 

To get started, I want to write about logistic regression, and more specifically the interpretation of the interaction term. I will limit my scope to 2 binary predictors $X_1$ and $X_2$. For better intuition, let's assign the predictors to fictional but realistic variables. We wish to find out whether **being a cat owner** is associated with the **location** and **sex**. 

- $X_1$ is the city (city A = 0, city B = 1)
- $X_2$ is the sex (female = 0, male = 1)
- $Y$ is whether the subject is a cat owner (cat owner = 1, no cat = 0)

Fitted model: 

$$logit(P(Y = 1)) = \beta_0 + \beta_1 city + \beta_2 sex + \beta_3city*sex$$

And let us assume the values of the parameters are $(-3.69, 1.83, 1.25, -1.89)$ respectively and they are all significant with p value less than 0.0001. So what does the result tell us? 



### Reminder on odds, odds ratio and probability 







|              | Sex = 0 (F)         | sex = 1 (M)                             |      |
| ------------ | ------------------- | --------------------------------------- | ---- |
| City = 0 (A) | $\beta_0$           | $\beta_0 + \beta_2$                     |      |
| City = 1 (B) | $\beta_0 + \beta_1$ | $\beta_0 + \beta_1 + \beta_2 + \beta_3$ |      |
|              |                     |                                         |      |





